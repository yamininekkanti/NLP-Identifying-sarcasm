# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uR9wyAyzBt3Sz7LrjiWNWNdE7GKT09y4
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from sklearn.preprocessing import LabelBinarizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud,STOPWORDS
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize,sent_tokenize
from bs4 import BeautifulSoup
import re,string,unicodedata
from keras.preprocessing import text, sequence
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
from string import punctuation
import keras
from keras.models import Sequential
from keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU
import tensorflow as tf

import os

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv(io.BytesIO(uploaded['Train.csv']))

df.head()

df.isna().sum()

sns.set_style("dark")
sns.countplot(df.label)

plt.figure(figsize = (20,20)) # Text that is Not Sarcastic
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.label == 0].comment))
plt.imshow(wc , interpolation = 'bilinear')

df['parent_comment']=df['parent_comment'].apply(denoise_text)

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
text_len=df[df['label']==1]['comment'].str.len()
ax1.hist(text_len,color='red')
ax1.set_title('Sarcastic text')
text_len=df[df['label']==0]['comment'].str.len()
ax2.hist(text_len,color='green')
ax2.set_title('Not Sarcastic text')
fig.suptitle('Characters in texts')
plt.show()

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
text_len=df[df['label']==1]['parent_comment'].str.len()
ax1.hist(text_len,color='red')
ax1.set_title('Sarcastic text')
text_len=df[df['label']==0]['parent_comment'].str.len()
ax2.hist(text_len,color='green')
ax2.set_title('Not Sarcastic text')
fig.suptitle('Characters in texts')
plt.show()

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))
word=df[df['label']==1]['comment'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')
ax1.set_title('Sarcastic text')
word=df[df['label']==0]['comment'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')
ax2.set_title('Not Sarcastic text')
fig.suptitle('Average word length in each text')

sub_df = df.groupby('topic')['label'].agg([np.size, np.mean, np.sum])
sub_df[sub_df['size'] > 300].sort_values(by='mean', ascending=False).head(10)

sub_df = df[df['score'] >= 0].groupby('score')['label'].agg([np.size, np.mean, np.sum])
sub_df[sub_df['size'] > 300].sort_values(by='mean', ascending=False).head(10)

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
text_len=df[df['label']==1]['down']
ax1.hist(text_len,color='red')
ax1.set_title('Sarcastic text')
text_len=df[df['label']==0]['down']
ax2.hist(text_len,color='green')
ax2.set_title('Not Sarcastic text')
fig.suptitle('Down votes')
plt.show()

train_texts, valid_texts, y_train, y_valid = \
        train_test_split(df['comment'], df['label'], random_state=17)

# build bigrams, put a limit on maximal number of features
# and minimal word frequency
tf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)
# multinomial logistic regression a.k.a softmax classifier
logit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', 
                           random_state=17, verbose=1)
# sklearn's pipeline
tfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), 
                                 ('logit', logit)])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tfidf_logit_pipeline.fit(train_texts, y_train)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# valid_pred = tfidf_logit_pipeline.predict(valid_texts)

accuracy_score(y_valid, valid_pred)

f1_score(y_valid, valid_pred)

def plot_confusion_matrix(actual, predicted, classes,
                          normalize=False,
                          title='Confusion matrix', figsize=(7,7),
                          cmap=plt.cm.Blues, path_to_save_fig=None):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    import itertools
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(actual, predicted).T
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    plt.figure(figsize=figsize)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('Predicted label')
    plt.xlabel('True label')
    
    if path_to_save_fig:
        plt.savefig(path_to_save_fig, dpi=300, bbox_inches='tight')

plot_confusion_matrix(y_valid, valid_pred, 
                      tfidf_logit_pipeline.named_steps['logit'].classes_, figsize=(8, 8))

subreddits = df['topic']
train_subreddits, valid_subreddits = train_test_split(subreddits, random_state=17)

tf_idf_texts = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)
tf_idf_subreddits = TfidfVectorizer(ngram_range=(1, 1))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# X_train_texts = tf_idf_texts.fit_transform(train_texts)
# X_valid_texts = tf_idf_texts.transform(valid_texts)

X_train_texts.shape, X_valid_texts.shape

# Commented out IPython magic to ensure Python compatibility.
# %%time
# X_train_subreddits = tf_idf_subreddits.fit_transform(train_subreddits)
# X_valid_subreddits = tf_idf_subreddits.transform(valid_subreddits)

X_train_subreddits.shape, X_valid_subreddits.shape

from scipy.sparse import hstack
X_train = hstack([X_train_texts, X_train_subreddits])
X_valid = hstack([X_valid_texts, X_valid_subreddits])

logit.fit(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# valid_pred = logit.predict(X_valid)

accuracy_score(y_valid, valid_pred)

f1_score(y_valid, valid_pred)

pc = df['parent_comment']
train_pc, valid_pc= train_test_split(pc, random_state=17)

tf_idf_texts = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)
tf_idf_pc = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# X_train_texts = tf_idf_texts.fit_transform(train_texts)
# X_valid_texts = tf_idf_texts.transform(valid_texts)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# X_train_subreddits = tf_idf_subreddits.fit_transform(train_subreddits)
# X_valid_subreddits = tf_idf_subreddits.transform(valid_subreddits)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# X_train_pc = tf_idf_pc.fit_transform(train_pc)
# X_valid_pc = tf_idf_pc.transform(valid_pc)

from scipy.sparse import hstack
X_train = hstack([X_train_texts, X_train_subreddits,X_train_pc])
X_valid = hstack([X_valid_texts, X_valid_subreddits,X_valid_pc])

logit.fit(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# valid_pred = logit.predict(X_valid)

f1_score(y_valid, valid_pred)

accuracy_score(y_valid, valid_pred)

from google.colab import files
uploaded = files.upload()

import io
df_t = pd.read_csv(io.BytesIO(uploaded['Test.csv']))

df_t.head()

test_texts = df_t['comment']
test_subreddits = df_t['topic']
test_pc = df_t['parent_comment']

X_train_texts = tf_idf_texts.fit_transform(train_texts)
X_test_texts = tf_idf_texts.transform(test_texts)

X_train_subreddits = tf_idf_subreddits.fit_transform(train_subreddits)
X_test_subreddits = tf_idf_subreddits.transform(test_subreddits)

X_train_pc = tf_idf_pc.fit_transform(train_pc)
X_test_pc = tf_idf_pc.transform(test_pc)

from scipy.sparse import hstack
X_train = hstack([X_train_texts, X_train_subreddits,X_train_pc])
X_test = hstack([X_test_texts, X_test_subreddits,X_test_pc])

logit.fit(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# test_pred = logit.predict(X_test)

len(test_pred)

test_pred

df_t['ID']

df_tp = pd.DataFrame({'label': test_pred})
 
df2 = pd.concat([df_t['ID'],df_tp], axis=1)

df2.head()

df2.tail()

df2.isna().sum()

df2.to_csv('submission',index=False)